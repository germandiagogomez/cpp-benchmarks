* Purpose

This is a set of benchmarks in C++ that tries
to compare "raw/C-ish code" implementations vs
"library-based, modern C++"
implementations of some algorithms and compares
their speeds.


For every benchmark,
two implementations are introduced:

- raw implementation.
- C++ modern implementation.

The goal is to put them front to front
to see how they perform against each other,
on a per-compiler basis.

Plots are generated, grouping, per-compiler,
the two versions put front to front.


I am particularly interested in measuring the abstraction
penalty incurred by the use of a C++ vs C-ish plain approaches
*when compiling programs with optimization*, since one
of the goals of C++ is to the zero-overhead principle.


My first experiment makes use of [[https://github.com/ericniebler/range-v3][Eric Niebler's ranges library]].
There is a standard C++ proposal for inclusion based on this work.

* A note about the benchmarks

The scope of this benchmark set is very targeted:
I want to show how typical, older-style or C-ish code
looks against idiomatic C++ code and how it performs.

There are plans to improve the information provided,
besides timing, about code safety and more in the
future, but that will need to wait for now.

I want to limit the benchmarks to code that is easy to
write for the average programmer, things that would be
likely to be written in the wild as a normal
attemp to perform an action, that look simple.

For example, if there is an I/O benchmark of C I/O vs C++ I/O,
I consider cheating to start to mess up with buffers and locales
and advanced tricks to improve performance, because
this is not what a normal programmer would write as a first attemp.

I plan to accept contributions, but for that, first,
I am making as easy as possible to contribute a benchmark.
I will reserve the right to accept or reject a benchmark
to the set of benchmarks, with the hope of keeping it focused.

* Contributing a new benchmark

TODO

* Compile and run the benchmarks

So you want to run the benchmark yourself in your computer...

Prerequisites:

- gnuplot.
- compilers to run the benchmarks against (only tested g++ and clang++ in OS X at this time).

NOTE: Support for Visual C++ benchmark execution is planned,
though, I cannot give a date for this. As of now only Unix-like systems
are supported, since my solution uses handmade makefiles.

#+BEGIN_src sh
git clone https://github.com/germandiagogomez/the-cpp-abstraction-penalty.git
git submodule init
git submodule update
./configure
cd build
make COMPILERS="clang++ g++"
#+END_src

This will:

1. Build the binaries for your compilers.
2. Run the binaries for benchmarking
3. Create a `plots` directory at the root and put the benchmarks .png files there,
  names benchmark-1.png for 1st benchmark, benchmark-2.png for 2nd benchmark,
  etc.


*WARNING*: Only tested on my Mac OSX computer.
* Benchmarks results

** Hardware information

I am using a =2,4 GHz Intel Core i5 4 GB 1600 MHz DDR3= with graphics
card =Intel Iris 1536 MB=. My OS is =OSX Yosemite 10.10.1=.


The versions of the compilers used for the benchmarks in my computer are:

   - g++-5 (Homebrew gcc5 5.2.0) 5.2.0.
   - Apple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)
     Target: x86_64-apple-darwin14.0.0
     Thread model: posix.

** Results


@BENCHMARKS_LIST@
- Sequential sieve algorithm.
   - [[./benchmarks/01-sieve/raw_sieve.cpp][Raw sieve]].
   - [[./benchmarks/01-sieve/ranges_sieve.cpp][Ranges Sieve]].


#+CAPTION: Sieve benchmark result.
#+NAME: fig:sieve-bench
[[./plots/benchmark-1.png]]
